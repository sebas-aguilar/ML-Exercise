{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in training and testing data and do some housekeeping:\n",
    "1. Remove duplicate rows\n",
    "2. Split both datasets into an n-dim feature/design matrix and 1-dim target/predictor vector\n",
    "3. Manually encode a value to the binary categorical labels of the target/predictor vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Problem while checking for missing/bad labels:\n",
      "Categories in `native-country` column of df_train not in df_test: {'Holand-Netherlands'}\n",
      "Categories in `native-country` column of df_test not in df_train: set()\n",
      "\n",
      "Problem while checking for missing/bad labels:\n",
      "Categories in `class` column of df_train not in df_test: {'<=50K', '>50K'}\n",
      "Categories in `class` column of df_test not in df_train: {'>50K.', '<=50K.'}\n",
      "\n",
      "\n",
      "# of entries before dropping Holand-Netherlands: 32561\n",
      "# of entries after dropping Holand-Netherlands: 32560\n",
      "\n",
      "\n",
      "# of rows in df_train before dropping unlabeled/missing values: 32560\n",
      "# of rows in df_train after dropping unlabeled/missing values: 30161\n",
      "\n",
      "\n",
      "# of rows in df_test before dropping unlabeled/missing values: 16281\n",
      "# of rows in df_test after dropping unlabeled/missing values: 15060\n",
      "\n",
      "\n",
      "Training predictor has 30161 samples and 7508 of those samples are classified as >50K.\n",
      "% of training sample >50K: 0.24893073837074367\n",
      "% of training sample <=50K: 0.7510692616292564\n",
      "\n",
      "\n",
      "Testing predictor has 15060 samples and 3700 of those samples are classified as `>50K`.\n",
      "% of testing sample >50K: 0.2456839309428951\n",
      "% of testing sample <=50K: 0.7543160690571049\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Read in the data, note: 'education-num' is a proxy for number of years in education\n",
    "dataTypes = {'age':np.uint8,'workclass':'category', 'fnlwgt':np.uint32, 'education':'category', 'education-num':np.uint8,\n",
    "             'marital-status':'category', 'occupation':'category', 'relationship':'category', 'race':'category', 'sex':'category', \n",
    "             'capital-gain':np.uint32, 'capital-loss':np.uint32, 'hours-per-week':np.uint8, 'native-country':'category',\n",
    "             'class':'category'}\n",
    "\n",
    "\n",
    "# Note, we have a mixture of categorical and numerical data, \n",
    "df_train = pd.read_csv('trainingData/au_train.csv', dtype=dataTypes, skipinitialspace=True, engine='c')\n",
    "df_test = pd.read_csv('testingData/au_test.csv', dtype=dataTypes, skipinitialspace=True ,engine='c')\n",
    "# print(df_train.head())\n",
    "# print(df_test.head())\n",
    "\n",
    "\n",
    "# Check for duplicate rows\n",
    "# print(df_train.drop_duplicates(inplace=True))\n",
    "# print(df_test.drop_duplicates(inplace=True))\n",
    "\n",
    "\n",
    "# Check some information and for bad values\n",
    "# print('\\nCheck some prelimiary info and for missing values/NaN:')\n",
    "# print(df_train.info(),df_test.info())\n",
    "# print(df_train.describe(),df_test.describe())\n",
    "\n",
    "\n",
    "# Check for unlabeled categorical features in our DataFrames / misspellings\n",
    "# print(sorted(list(df_train['workclass'].unique())))\n",
    "# print(sorted(list(df_train['occupation'].unique())))\n",
    "# print(sorted(list(df_train['native-country'].unique())))\n",
    "# print(sorted(list(df_test['workclass'].unique())))\n",
    "# print(sorted(list(df_test['occupation'].unique())))\n",
    "# print(sorted(list(df_test['native-country'].unique())))\n",
    "\n",
    "\n",
    "# Check for misspelled labels\n",
    "categories = ['workclass','education','marital-status','occupation','relationship','race','sex','native-country','class']\n",
    "for cat in categories:\n",
    "    a = set(df_train[cat].unique())\n",
    "    b = set(df_test[cat].unique())\n",
    "    if len(a.difference(b)):\n",
    "        print('\\nProblem while checking for missing/bad labels:')\n",
    "        print('Categories in `%s` column of df_train not in df_test:'%cat,a.difference(b))\n",
    "        print('Categories in `%s` column of df_test not in df_train:'%cat,b.difference(a))\n",
    "# Note: The test sets target has an extraneous '.' at the end. Handle this manually later\n",
    "# In addition there is no entry for 'Holand-Netherlands' in the test set so you could argue that data \n",
    "# to train on may not be needed\n",
    "print('\\n\\n# of entries before dropping Holand-Netherlands:',len(df_train))\n",
    "# df_train = df_train.replace('Holand-Netherlands',-1)\n",
    "# print(df_train.query('`native-country` == -1',engine='python'))\n",
    "df_train = df_train.replace('Holand-Netherlands',np.nan).dropna()\n",
    "print('# of entries after dropping Holand-Netherlands:',len(df_train))\n",
    "\n",
    "\n",
    "\n",
    "# Have some unlabeled/missing values (i.e '?') of categorical features in: \n",
    "#                'workclass','occupation','native-country'\n",
    "# in both our training and testing DataFrame (categorical), need to drop those\n",
    "print('\\n\\n# of rows in df_train before dropping unlabeled/missing values:',len(df_train))\n",
    "df_train = df_train.replace('?',np.nan).dropna()\n",
    "print('# of rows in df_train after dropping unlabeled/missing values:',len(df_train))\n",
    "print('\\n\\n# of rows in df_test before dropping unlabeled/missing values:',len(df_test))\n",
    "df_test = df_test.replace('?',np.nan).dropna()\n",
    "print('# of rows in df_test after dropping unlabeled/missing values:',len(df_test))\n",
    "\n",
    "\n",
    "# Split our data into 'X' commonly referred to as the 'Design Matrix' which contains our \n",
    "# 'predictors/features' and 'y' containing our prediction labels\n",
    "y_train = df_train['class'].to_numpy()\n",
    "X_train = df_train.drop(columns=['class'])\n",
    "\n",
    "y_test = df_test['class'].to_numpy()\n",
    "X_test = df_test.drop(columns=['class'])\n",
    "\n",
    "\n",
    "# Manually encode (easy to do for a 1d array) a numerical value for our predictor which had\n",
    "# a binary value\n",
    "#            <=50K  --->    0\n",
    "#             >50K  --->    1\n",
    "y_train = np.where(y_train=='>50K',1,0)\n",
    "y_test = np.where(y_test=='>50K.',1,0)\n",
    "\n",
    "\n",
    "# Check to see the distribution of the predictors in both sets\n",
    "print('\\n\\nTraining predictor has',len(y_train),'samples and',np.sum(y_train),'of those samples \\\n",
    "are classified as >50K')\n",
    "print('% of training sample >50K:',np.sum(y_train)/len(y_train))\n",
    "print('% of training sample <=50K:',1-np.sum(y_train)/len(y_train))\n",
    "print('\\n\\nTesting predictor has',len(y_test),'samples and',np.sum(y_test),'of those samples \\\n",
    "are classified as `>50K`')\n",
    "print('% of testing sample >50K:',np.sum(y_test)/len(y_test))\n",
    "print('% of testing sample <=50K:',1-np.sum(y_test)/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: There is more data for people making  <=50K\n",
    "\n",
    "## Poke around the data to see if any of the features are skewed or may need scaling\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "groupbySex = X_test.groupby('sex')\n",
    "colorDict = {'Male':'b','Female':'r'}\n",
    "for sex,group in groupbySex:\n",
    "#     print(sex)\n",
    "#     print(group.describe(),'\\n\\n')\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "    fig.patch.set_facecolor('xkcd:gray')\n",
    "    fig.suptitle(sex)\n",
    "    group.plot(x='capital-gain',y='age',kind='scatter',ax=axes[0,0], c=colorDict[str(sex)],label=sex,legend=True,alpha=.4)\n",
    "    group.plot(x='capital-gain',y='education-num',kind='hexbin',ax=axes[0,1],gridsize=15,cmap='viridis',norm=LogNorm())\n",
    "    group.plot(x='capital-loss',y='age',kind='scatter',ax=axes[1,0],c=colorDict[str(sex)],label=sex,legend=True,alpha=.4)\n",
    "    group.plot(x='capital-loss',y='education-num',kind='hexbin',ax=axes[1,1],gridsize=15,cmap='viridis',norm=LogNorm())\n",
    "\n",
    "groupbyRace = X_test.groupby('race')\n",
    "colorDict = {'White':'r','Asian-Pac-Islander':'g','Amer-Indian-Eskimo':'b','Other':'c','Black':'m'}\n",
    "for race,group in groupbyRace:\n",
    "#     print(sex)\n",
    "#     print(group.describe(),'\\n\\n')\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n",
    "    fig.patch.set_facecolor('xkcd:gray')\n",
    "    fig.suptitle(race)\n",
    "    group.plot(x='capital-gain',y='age',kind='scatter',ax=axes[0,0], c=colorDict[str(race)],label=sex,legend=True,alpha=.4)\n",
    "    group.plot(x='capital-gain',y='education-num',kind='hexbin',ax=axes[0,1],gridsize=15,cmap='viridis',norm=LogNorm())\n",
    "    group.plot(x='capital-loss',y='age',kind='scatter',ax=axes[1,0],c=colorDict[str(race)],label=sex,legend=True,alpha=.4)\n",
    "    group.plot(x='capital-loss',y='education-num',kind='hexbin',ax=axes[1,1],gridsize=15,cmap='viridis',norm=LogNorm())\n",
    "    \n",
    "    \n",
    "# Plot correlation matrix\n",
    "labelNames = [numFeat for numFeat in X_train.select_dtypes(include=np.number)]\n",
    "corr = X_train[labelNames[:]].corr()\n",
    "\n",
    "\n",
    "im = plt.matshow(corr,cmap='viridis',interpolation='bilinear',norm=plt.LogNorm())\n",
    "plt.xticks(np.arange(corr.shape[1]),labels=labelNames,rotation=45,va='bottom',ha='left')\n",
    "plt.yticks(np.arange(corr.shape[0]),labels=labelNames)\n",
    "plt.colorbar(im)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data is still heterogeneous(mixed numerical and categorical data), *scikit-learn* requires explicit conversion of categorical features to numeric values (**preprocessing**).\n",
    "1. Possibility to scale the numeric features (helpful sometimes i.e convergence)\n",
    "2. Categorical data will be *OneHotEncoded* to guarantee model sensitivity only to the original category labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "# Create the preprocessing pipelines for categorical data of training set.\n",
    "train_numeric_features = [numFeat for numFeat in X_train.select_dtypes(include=np.number)]\n",
    "train_numeric_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', RobustScaler())])\n",
    "\n",
    "train_categorical_features = [catFeat for catFeat in X_train.select_dtypes(include=\"category\")]\n",
    "train_categorical_transformer = Pipeline(steps=[\n",
    "#     ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', train_numeric_transformer, train_numeric_features),\n",
    "        ('cat', train_categorical_transformer, train_categorical_features)],\n",
    "        remainder='passthrough') # Don't drop untransformed columns\n",
    "\n",
    "preprocessorNoScale = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', train_categorical_transformer, train_categorical_features)],\n",
    "        remainder='passthrough') # Don't drop untransformed columns\n",
    "\n",
    "# Create pipelines with different scales\n",
    "pipe = Pipeline(steps=[('preprocessor', preprocessor)]) # RobustScale\n",
    "pipeNoScale = Pipeline(steps=[('preprocessor', preprocessorNoScale)])\n",
    "\n",
    "X_trainTrans = X_train.copy()\n",
    "X_testTrans = X_test.copy()\n",
    "X_trainTransNoScale = X_train.copy()\n",
    "X_testTransNoScale = X_test.copy()\n",
    "\n",
    "# Transform the sets\n",
    "X_trainTrans = pipe.fit_transform(X=X_trainTrans,y=y_train)\n",
    "X_testTrans = pipe.transform(X=X_testTrans)\n",
    "X_trainTransNoScale = pipeNoScale.fit_transform(X=X_trainTransNoScale,y=y_train)\n",
    "X_testTransNoScale = pipeNoScale.transform(X=X_testTransNoScale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data has been preprocessed and transformed, lets play with some models\n",
    "\n",
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr score for scaling of features: 0.37008814785028715\n",
      "lr score for no scaling of features: 0.36926789988688424\n",
      "[[19880  2773]\n",
      " [ 2856  4652]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Shuffle training set in case model is sensitive to order\n",
    "# np.random.seed(42)\n",
    "# shuffle_index = np.random.permutation(len(X_trainTrans))\n",
    "# X_trainTrans, y_train = X_trainTrans[shuffle_index], y_train[shuffle_index]\n",
    "\n",
    "# RobustScaled\n",
    "lr_clf = LinearRegression()\n",
    "lr_clf.fit(X=X_trainTrans,y=y_train,sample_weight=None)\n",
    "lr_pred = lr_clf.predict(X_testTrans)\n",
    "print('lr score for scaling of features:',lr_clf.score(X=X_trainTrans,y=y_train,sample_weight=None))\n",
    "\n",
    "# No scaling of features\n",
    "lr_clf.fit(X=X_trainTransNoScale,y=y_train,sample_weight=None)\n",
    "lr_pred = lr_clf.predict(X_testTransNoScale)\n",
    "print('lr score for no scaling of features:',lr_clf.score(X=X_trainTransNoScale,y=y_train,sample_weight=None))\n",
    "# lr score is R^2 value -> Not a good model \n",
    "\n",
    "\n",
    "# Using our model create array holding its predicitions\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_trainTrans, y_train, cv=5)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "CM = confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "im = plt.matshow(CM,cmap='viridis',norm=LogNorm())\n",
    "plt.xticks(np.arange(0,2),labels=['<=50K','>50K'],rotation=45,va='bottom',ha='left')\n",
    "plt.colorbar(im)\n",
    "\n",
    "TP = CM[1][1]\n",
    "TN = CM[0][0]\n",
    "FP = CM[0][1]\n",
    "FN = CM[1][0]\n",
    "recall = TP/(TP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "F_score = 2*recall*precision/(recall + precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logr score: 0.8479824939491396\n",
      "logr score: 0.7909220516561122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# RobustScaled features\n",
    "logr_clf = LogisticRegressionCV(cv=3,random_state=42,solver='lbfgs',max_iter=1000)\n",
    "logr_clf.fit(X=X_trainTrans,y=y_train,sample_weight=None)\n",
    "logr_pred = logr_clf.predict(X_testTrans)\n",
    "print('logr score:',logr_clf.score(X=X_trainTrans,y=y_train,sample_weight=None))\n",
    "\n",
    "# No scale features\n",
    "logr_clf.fit(X=X_trainTransNoScale,y=y_train,sample_weight=None)\n",
    "logr_pred = logr_clf.predict(X_testTransNoScale)\n",
    "print('logr score:',logr_clf.score(X=X_trainTransNoScale,y=y_train,sample_weight=None))\n",
    "\n",
    "# lr score is mean accuracy value -> Getting better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD score: 0.8130146082337317\n",
      "\n",
      "Average accuracy after 5-fold cross validation:\n",
      "[0.79827615 0.80852122 0.81780504 0.81846817 0.78680371]\n",
      "[[19792  2861]\n",
      " [ 2807  4701]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(loss='hinge',max_iter=1000,shuffle=True)#,random_state=42)\n",
    "sgd_clf.fit(X_trainTrans,y_train,sample_weight=None)\n",
    "sgd_pred = sgd_clf.predict(X_testTrans)\n",
    "print('SGD score:',sgd_clf.score(X_testTrans,y_test)) # This is accuracy in getting the label correct\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print('\\nAverage accuracy after 5-fold cross validation:')\n",
    "print(cross_val_score(sgd_clf, X_trainTrans, y_train, cv=5,scoring='accuracy')) # Mediocre improvement using linear SVM model\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_trainTrans, y_train, cv=5)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "CM = confusion_matrix(y_train, y_train_pred)\n",
    "print(CM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svm_clf = svm.SVC()\n",
    "svm_clf.fit(X_train, y_train)\n",
    "svm_pred = svm_clf.predict(X_test)\n",
    "print('SVM score:',svm_clf.score(X_test,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
